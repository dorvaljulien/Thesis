%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Chapitre 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{The Hubble-Lema\^itre fragmented model} 
\label{ChapterHL}

\section{How to build a Hubble-Lema\^itre model}

\subsection{Initial state}

The first step to obtain a HL-fragmented model is to build an uniform sphere model. The N stars, depending on the required membership, have to be distributed randomly in space inside a certain radius, producing an uniform density. This can be achieved by sampling separately the distance to the center and the angular position of each star, in a method analog as used in \cite{Aarseth1974} for a Plummer model. The distance to the center should be sampled from the function:

\begin{equation}
f_R(X) = R_0 X^2
\end{equation} 

With $R_0$ the bouding radius and X a random variable following a uniform probability law between 0 and 1. A direct uniform law for the radius would overpopulate the outer regions. The angles $\phi$ and $\theta$, respectively azimuthal and polar angle in the physics convention, should be sampled from:


\begin{align}
f_\phi(X_1) & = 2\pi X_1\\
f_\theta(X_2) &= \arccos{ (X_2) }
\end{align}

With $X_1$ following a uniform probability law between 0 and 1 and $X_2$ between -1 and 1. The cartesian coordinates are then found:

\begin{align}
x &= R \sin{\theta} \cos{\phi}\\
y &= R \sin{\theta} \sin{\phi}\\
z &= R \cos{\theta} \\
\end{align}

The N particles are then homogeneously distributed in space in a sphere of radius $R_0$. The next step is to attribute velocities. Unlike other models like the Plummer model, the velocities are here straightforward. We use the well known velocity field of neighbouring galaxies: velocities are radial from the Milky Way, larger with increasing distances, taking the form:

\begin{equation}
\label{Eq:1_Hubble}
\bold{v} =  \textrm{H}_0 \bold{r},
\end{equation}

with H$_0$ being an equivalent of the well-known Hubble parameter. For historical accuracy, I added the name of Georges Lema\^itre when I named my model. It has now been shown that the astronomical observations of redshifted galaxies and its interpretation as the consequence of an expanding universe predated Hubble's paper \citep{Hubble1929}. Georges Lema\^itre had published his conclusion on an expanding universe two years earlier \citep{Lemaitre1927}. The account of this can be found in \cite{Kragh2003,VanDenBergh2011} and \cite{Freeman2015}.

An appropriate H$_0$ to obtain a fragmented subvirial model has to be inferior to 1.4 (see next section). The model obtained from this is then evolved through a nbody integrator, which in my case is NBODY6.


\subsection{Fragmentation}

The cluster expands, driven by the initial Hubble-Lema\^itre velocity field. During this expansion, poissonian fluctuation in density from the uniform model starts to grow: the part of the cluster with more mass initially attract more stars, forming clumps, clumps merge, spontaneously building substructure. These clumps will be analyzed in another section. If H$_0$ is well chosen, the expansion stops at some point, the apex, at which the initial kinetic energy has been spent and converted to potential energy: the cluster is now larger, substructured and subvirial, about to collapse. The apex time $t_a$ of the end of the expansion and the critical value of H$_0$ can be derived from Newton's second law applied to an expanding spherical shell of matter.

We start from a uniform sphere of radius $R_0$, total mass $M$. We consider spherical shells as mass elements, situated at distance $r$ from the origin. As previously said, they are attributed a radial velocity following (for the shell at $r=R_0$) $\vec v_0 = \Hub_0 \vec R_0 = \Hub_0 R_0 \vec u_r$. We want to follow the radial motion of the last shell of mass m, situated at $R$ from the origin. Newton's second law gives:


%\begin{equation}
\begin{align}\label{eq:newton}
m \frac{dv}{dt} & = - \frac{G M m}{R^2}
\end{align}
%\end{equation}

By multiplying on both sides by $v$ and integrating between a given time and $t=0$, one finds:

\begin{equation}
v^2(t) - v^2_0 = 2GM \left( \inv{R} - \inv{R_0} \right)
\end{equation}

Which becomes, by taking $\nu = v/v0$,  $x= R/R0$ and $E_\ast = \frac{2GM}{R_0 v_0^2}$, which is a dimensionless measure of the total energy of the system:

\begin{equation}
\nu^2  = 1 + E_\ast \left( \inv{x} - 1 \right) .
\end{equation}

The evolution of the system has 3 outcomes, depending on the value of $E_\ast$:
\begin{itemize}
\item $E_\ast<1$ The velocity is always strictly positive as the system expands ($x->\infty$). The system is unbound.
\item $E_\ast=1$ The velocity approaches zero as the system expands. The expansion "stops at an infinite radius". The system is marginally bound.
\item $E_\ast>1$ The velocity reaches zero for a finite radius, the system is bound and will collapses back on itself once the expansion stops. 
\end{itemize}

Using H\'enon units, $G=1$ and $M=1$, and we choose $R_0$=1. Which gives a critical value $E_*$ to have a bound system: $E_* = \frac{2}{\Hub^2_0} < 1$. This means to have a bound system, which stops expanding at some point, one must have $\Hub_0 < \sqrt{2}$.
We only consider in the following the case in which $E_\ast<1$. We have the expression

\begin{equation}
\nu = \sqrt{1+E_\ast\left(\inv{x} - 1\right)}
\end{equation}

Taking the time derivative gives:

\begin{equation}
\frac{d \nu}{dt} = - \frac{E_\ast}{2 x^2} \left[ 1 + E_\ast\left(\inv{x} -1\right)\right]^{-\frac{1}{2}} \frac{dx}{dt}
\end{equation}

Combining this with (\ref{eq:newton}), one obtains:

\begin{equation}
\frac{dx}{dt} = \Hub_0 \sqrt{1+ E_\ast\left( \inv{x} -1\right)}
\end{equation}

which can be rewritten, using $\tilde{\Hub_0} = \Hub_0 \sqrt{E_\ast-1}$ and $x_t=\frac{E_\ast}{E_\ast-1}$

\begin{equation}
\frac{dx}{dt} = \tilde{\Hub_0} \sqrt{\frac{x_t}{x}-1}
\end{equation}

$x_a$ being the extent of the maximum expansion as we assumed a bound system. The subscript a is for apex. If we choose the notation $u = \frac{x}{x_a}$:

\begin{equation}
\sqrt{\frac{u}{u-1}} \frac{du}{dt} = \frac{\tilde{\Hub_0}}{x_a}
\end{equation}

We know that $x$ varies from 1 to $x_a$, thus $u$ varies from $1/x_a$ to 1. We can then make the change of variable $u = \sin^2\theta$ and separate the variables:

\begin{equation}
\sqrt{\frac{\sin^2\theta}{1-\sin^2\theta}} 2 \sin\theta \cos\theta d \theta = \frac{\tilde{\Hub_0}}{x_a} dt
\end{equation}

which becomes after simplifications:

\begin{equation}
[ 1 - \cos(2\theta)]d\theta = \frac{\tilde{\Hub_0}}{x_a} dt .
\end{equation}


We now integrate the expression from $t=0$ to $t$, the time at which the expansions stops and $x$ reaches $x_a$ (wich implies $u_a = 1$ and $\theta_a = \pi /2)$:

\begin{align}
\int^{\pi/2}_{\theta_0} [ 1 - \cos(2\theta)]d\theta  & = \int^t_0 \frac{\tilde{\Hub_0}}{x_a} dt\\
\frac{\pi}{2} - \theta_0 + \frac{\sin(2\theta_0)}{2} & =  \frac{\tilde{\Hub_0}}{x_a} t\\
\pi - 2 \theta_0 + \frac{2}{\sqrt{x_a}}\sqrt{1-\inv{x_a}} & = 2 \frac{\tilde{\Hub_0}}{x_a} t 
\end{align}

which boils down to the expression of the time at which the expansion stops:

\begin{equation}
t_a = \frac{E_\ast \left(\frac{\pi}{2} - \theta_0\right) + \sqrt{E_\ast-1}}{\Hub_0 (E_\ast-1)^{-\frac{3}{2}}}.
\end{equation}

Recalling the quantities:
\begin{align}
E_\ast = \frac{2GM}{R_0 v_0^2}        &;  & x_a=\frac{E_\ast}{E_\ast-1}  &;  &\theta_0 = \sin^{-1}\left(\inv{\sqrt{x_a}}\right)  
\end{align}

See figure \ref{Fig:apextime} for the value of $t_a$ as a function of \Hub$_0$

\begin{figure}
\center
\includegraphics[width=0.7\linewidth]{Figures/1_apextime.png}
\caption{Theoretical values of the apex time, at which the system stops expanding, as a function of initial HL parameter, which tunes the strength of the initial expansion.}
\label{Fig:apextime}
\end{figure} 





\section{Perturbation theory}


\section{Analysis of the fragmented system}

\subsection{Clump finding algorithm}



\begin{figure}
\begin{center}
\includegraphics[width=0.8\columnwidth]{Figures/1_MST.png}
\end{center}
\caption{Illustration of a Minimum Spanning Tree and its use to isolate subgroups, using a cutting length $d_{cut}$.}
\label{Fig:1_MST}
\end{figure}

The study of substructures requires an efficient clump-identification algorithm (or, {\it halo-finding} in cosmology). By clump we mean here a local overdensity of stars. Several methods are commonly used such as the HOP algorithm \citep{Eisenstein1998,Skory2010} which relies on attributing local densities to each particle and separating the clumps through density thresholds. The HOP algorithm is very robust on large cosmological data sets. However, our calculations have comparatively coarse statistics and noisy density fields. This issue, coupled with the  large number of free parameters of the HOP algorithm, makes the method less appealing. Instead we follow \cite{Maschberger2010} who adapted the minimum spanning tree (MST~; see e.g. \citealt{Allison2009b,Olczak2011}) technique to the detection of clumps. A spanning tree is a set of edges connecting a group of  particles but without closed loops~; the MST seeks to minimise the total length of the edges. One may then construct the MST for the whole system, and then delete all edges larger than a chosen cutting length, $d_{cut}$. The sub-sets that are still connected  are labeled as clumps. This process is illustrated in Fig \ref{Fig:1_MST}. In practice a minimum sub-set size $N_d$  is also chosen so as to avoid many small-N subgroups~: experience led us to choose  $N_d = 12$ for the minimum number of stars per clump. 




With $N_d$ fixed, the length $d_{cut}$ is then the only free parameter left. There is some freedom 
in choosing an appropriate value. 
\cite{Maschberger2010} fixed the value of  $d_{cut}$ by visual inspection of clumps.
 We instead  identified  clumps in a fragmented system for a range of values for $d_{cut}$ and settled for the value  which optimised the number of identifications. This is shown on Fig.~\ref{Fig:Ndcut} for an N = 80k fully-fragmented Hubble model. For small $d_{cut}$'s, the number of detected clumps at first  increases rapidly. The rise is due  to the length $d_{cut}$ initially being small compared with the typical volume spawned by $N_d$ or more  nearest-neighbours. Beyond a certain value, a transition to another regime occurs, whereby the algorithm starts to connect previously separated clumps, counting them as one. The number of clumps thereafter begins to decrease. The value $d_{cut} \approx 0.025$ H.u optimises the outcome of the clump-search. This is a generic feature of the MST algorithm and we have adopted the same strategy throughout, adapting the value of $d_{cut}$ to the number $N$ of stars used. 
 
 
\begin{figure}
\begin{center}
\includegraphics[width=0.6\columnwidth]{Figures/1_Ndcut.png}
\end{center}
\caption{}
\label{Fig:1_Ndcut}
\end{figure} 
 
   On Fig.~\ref{Fig:1_clumpsABC}, a sub-set of the model is shown where we have identified stars that belong to clumps with filled symbols. The three panels on that figure are each for a different value of $d_{cut}$, increasing from top to bottom. For the smallest value $d_{cut}$=0.015 H.u, clumps look somewhat truncated as we are still in the under-sampling regime and only their cores registered as clumps. The second, optimal, value $d_{cut}$=0.025 H.u produces visually well-isolated clumps. Finally, the third and  largest value is so that clumps begin to merge together~: this is shown by the unique clump identified in the bottom panel (filled blue squares).
   


\begin{figure}
\begin{center}
\includegraphics[width=0.9\columnwidth]{Figures/1_clumpsABC.png}
\end{center}
\caption{}
\label{Fig:1_clumpsABC}
\end{figure} 


The procedure developed here gives results in agreement with other clump-identification algorithms developed using the MST (see e.g. \citealt{Gutermuth2009,Kirk2011}).





